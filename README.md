# ğŸ§ª Awful Book Sanitizer: Transforming Chaos into Clarity  

> *A Rust program that turns OCR-mangled books into readable, sane text. Because nobody wants to read the *literal* results of a neural network.*  

---

## ğŸ“š What Is This?  

This is **`awful_book_sanitizer`**, a command-line tool designed to clean up text excerpts from books that were *too spooky* for OCR.  

**Key features:**  
- **Asynchronous processing** with multiple configurations (for different LLMs/APIs).  
- **Chunked text splitting** to avoid overwhelming models.  
- **YAML output format**, so you can later analyze sanity or just read the text.  
- **Exponential backoff** to handle API failures like a seasoned ghostbuster.  

*Despite its ominous name, it's actually pretty awesome.*  

---

## ğŸ§© How It Works

1. **Input**: A directory of `.txt` files (probably from OCR).  
2. **Chunk It Up**: Split text into 500-token chunks (a number chosen because it felt right).  
3. **Send to LLM**: Use a conversational template (like "You are a librarian who fixes typos") to ask the model to *sanitize* the text.  
4. **Output**: YAML files with chunks of clean text (or nope, if the API threw a tantrum).  

*Think of it as a magical wand that turns "This is a really bad word" into "That's actually the correct spelling."*

---

## ğŸ§ª Example Usage  

```bash
awful_book_sanitizer --i /path/to/ocr-books --o /path/to/output --c llama-cpp-config.yaml google-colab-config.yaml
```

**What this does**:  
- `-i` = input directory (e.g., "ocr_books").  
- `-o` = output directory (will create YAML files like `book1.yaml`).  
- `-c` = configuration files for different LLMs.  

**Bonus tip**: If your books are "too spooky," try adding `$RUST_LOG=info` to see the program's internal monologue.  

---

## ğŸ§  Architecture (In A Nutshell)  

- **Text Splitter**: Divides text into chunks of 500 tokens (like a greedy librarian).  
- **Template Engine**: Sends prompts to the LLM (e.g., "You are a librarian who fixes typos").  
- **Async Threads**: Processes multiple configurations simultaneously (like having 20 assistants at once).  

*This is not just a sanitizerâ€”it's a high-stakes collaboration between humans and AI.*  

---

## ğŸ§‘â€ğŸ’» Contributing & Feedback  

- **Report bugs**: The program's "sanity" isn't guaranteed. If your output is *too* sane, something went wrong.  
- **Suggest improvements**: We're a team of "empathetic sociopaths" trying to make the best of things.  
- **Share your data**: If you have OCR-mangled books, submit them (but only if theyâ€™re not spooky).  

*Just remember: The goal isnâ€™t to fix every typo. Itâ€™s to make sure your books are at least *legible*.  

---

## ğŸ“Œ Credits  

- **Core Authors**: Thomas Gentry <thomas@awulsec.com> (the human cranking the flywheel that makes this stuff)
- **Rust Engine**: Written in Rust with `tokio`, `serde`, and `clap`.  
- **LLM Templates**: Based on OpenAI-compatible endpoints (e.g., `llama-cpp @ localhost` or `Google Colab`).  
- **Inspiration**: All the bad OCR results you've ever encountered.  

---

## ğŸ§  Fun Fact  

The hardest part about finetuning an LLM is managing all of the input and output files. ğŸ¤”

*Youâ€™re welcome to email me with creative solutions.*  

--- 

## ğŸ§ª Want to Try It?  

1. **Install dependencies**:  
   ```bash
   cargo install awful-book-sanitizer
   ```

2. **Run it**:  
   ```bash
   awful_book_sanitizer -i books -o output -c configs.yaml
   ```

3. **Check your YAML files**:  
   ```bash
   cat results/book1.yaml
   ```

*If the text looks *too* clean, you might have a problem. But if it's readable, great!*  

--- 

## ğŸ§  Final Thoughts  

This program is a love letter to bad OCR and the power of LLMs. Itâ€™s not perfect, but itâ€™s a step toward *sanity* for your books.  

**Remember**: The goal isnâ€™t to make the text perfectâ€”itâ€™s to make it *useful enough*.  

*Now go forth and sanitize!* ğŸ§ªğŸ“š
